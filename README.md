<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white" />
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" />
  <img src="https://img.shields.io/badge/React_19-61DAFB?style=for-the-badge&logo=react&logoColor=black" />
  <img src="https://img.shields.io/badge/Vite_6-646CFF?style=for-the-badge&logo=vite&logoColor=white" />
  <img src="https://img.shields.io/badge/SQLite-003B57?style=for-the-badge&logo=sqlite&logoColor=white" />
  <img src="https://img.shields.io/badge/WebSocket-010101?style=for-the-badge&logo=socket.io&logoColor=white" />
</p>

# âš¡ FluxCLI â€” Dynamic ETL Pipeline & Real-Time Dashboard

> **A CLI-first ETL pipeline that evolved into a full-stack web platform** â€” now supporting **any CSV schema** with automatic column detection, real-time pipeline monitoring via WebSocket, interactive analytics dashboards, and bilingual i18n support (EN/ES).

---

## ğŸ¯ Purpose

FluxCLI automates the processing of CSV data through a robust **Extract â†’ Transform â†’ Load â†’ Notify** pipeline. Originally built as a Python CLI tool for sales data, it was migrated to a modern web platform and later generalized to accept **any CSV schema dynamically**. It provides:

- **Dynamic schema detection** â€” automatically identifies column types (`numeric`, `date`, `text`) from any CSV
- **Real-time visibility** into ETL execution via WebSocket-streamed logs
- **Interactive dashboards** with data analytics, quality metrics, and execution history
- **Data governance** through a quarantine system that isolates invalid records for review
- **Automated notifications** via Email and Slack upon pipeline completion

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     React 19 + Vite 6           â”‚      â”‚        FastAPI Backend            â”‚
â”‚                                 â”‚      â”‚                                  â”‚
â”‚  Dashboard â”€ Analytics          â”‚ HTTP â”‚  /pipeline/run  â†’ PipelineRunner â”‚
â”‚  DataExplorer â”€ History   â—„â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â–º /data/*        â†’ SQLite queries â”‚
â”‚  Quarantine                     â”‚  WS  â”‚  /data/schema   â†’ Column types   â”‚
â”‚                                 â”‚      â”‚  /data/export   â†’ CSV download   â”‚
â”‚  i18n (EN/ES) â”€ Recharts        â”‚      â”‚  /data/reset    â†’ Full cleanup   â”‚
â”‚  Datrix Logo                    â”‚      â”‚  /ws/{run_id}   â†’ Live logs      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚                                  â”‚
                                         â”‚  Services:                       â”‚
                                         â”‚   â”œâ”€ pipeline_runner.py          â”‚
                                         â”‚   â””â”€ log_handler.py (WS bridge)  â”‚
                                         â”‚                                  â”‚
                                         â”‚  Core ETL:                       â”‚
                                         â”‚   â”œâ”€ extractor.py                â”‚
                                         â”‚   â”œâ”€ transformer.py (dynamic)    â”‚
                                         â”‚   â”œâ”€ loader.py (JSON datasets)   â”‚
                                         â”‚   â””â”€ notifier.py (Email/Slack)   â”‚
                                         â”‚                                  â”‚
                                         â”‚  Data Layer:                     â”‚
                                         â”‚   â”œâ”€ datasets (JSON rows)        â”‚
                                         â”‚   â”œâ”€ dataset_schema (col types)  â”‚
                                         â”‚   â””â”€ pipeline_runs (history)     â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¥ The Migration Challenge

This project wasn't just "add a frontend." It was a **full architectural transformation** from a synchronous CLI script to an async, event-driven web platform â€” and later, from a fixed-schema pipeline to a **fully dynamic ETL engine**. Here's what made it hard:

### ğŸ§© Synchronous â†’ Asynchronous Execution

The original pipeline blocks the process for 30+ seconds during execution. In a web context, that means HTTP timeouts and frozen UIs. **Solution:** Rewrote the orchestration layer using `asyncio.to_thread()`, returning a `run_id` immediately and streaming progress via WebSocket.

### ğŸ“¡ stdout â†’ WebSocket Log Streaming

The CLI printed logs to the terminal. The web dashboard needed them in real time. **Solution:** Built a custom `logging.Handler` that intercepts log messages and broadcasts them to connected WebSocket clients â€” supporting multiple concurrent sessions per `run_id`.

### ğŸ“ File Paths â†’ HTTP Uploads

The CLI receives `--file path/to/file.csv`. Browsers don't work that way. **Solution:** Implemented `multipart/form-data` upload with an auto-detect mode that scans for today's file in the `data/` directory.

### ğŸ—ƒï¸ Ephemeral â†’ Persistent State

The CLI had no memory â€” each run was fire-and-forget. The dashboard needs execution history, metrics, and trends. **Solution:** Designed a `pipeline_runs` table in SQLite tracking `total_read`, `total_valid`, `total_rejected`, `db_inserts`, `db_updates`, `duration`, and `status`.

### ğŸ”„ Fixed Schema â†’ Dynamic Schema

The original pipeline was hardcoded for a 6-column sales CSV (`id`, `date`, `product`, `qty`, `price`, `store_id`). **Solution:** Rewrote the transformer with `ColumnSchema` and `TransformResult` dataclasses that auto-detect column types (`numeric`, `date`, `text`). The loader now stores data as JSON rows in a generic `datasets` table with a companion `dataset_schema` table â€” enabling any CSV to be processed without code changes.

### ğŸ¼ Pandas 3.0 Breaking Change

Pandas 3.0 changed how `NaN` values behave â€” they no longer auto-cast to strings. This caused a subtle `AttributeError: float has no attribute 'lower'` deep in the transformer. A single missing `str()` wrapper was the fix, but finding it required tracing through real production data.

### ğŸ¨ Two UI Rewrites

The first frontend used glassmorphism (blur, gradients, heavy shadows). It didn't match the target Flatkit design language. **The entire CSS was rewritten** to a flat, clean aesthetic with subtle borders and minimal shadows.

<details>
<summary><b>ğŸ“Š Full Issue Tracker (11 problems solved)</b></summary>

| # | Problem | Severity | Category |
|---|---------|----------|----------|
| 1 | Blocking synchronous execution | ğŸ”´ High | Architecture |
| 2 | Logs to stdout with no web channel | ğŸ”´ High | Architecture |
| 3 | File upload vs local paths | ğŸŸ¡ Medium | Architecture |
| 4 | No execution history | ğŸŸ¡ Medium | Architecture |
| 5 | Pandas 3.0 NaN + `.lower()` | ğŸ”´ High | Compatibility |
| 6 | Vite scaffolding failure | ğŸŸ¡ Medium | Compatibility |
| 7 | CORS cross-origin blocking | ğŸŸ¡ Medium | Compatibility |
| 8 | Recharts tooltip unreadable on dark theme | ğŸŸ¢ Low | UI/UX |
| 9 | Sidebar active color mismatch | ğŸŸ¢ Low | UI/UX |
| 10 | Glassmorphism vs flat design mismatch | ğŸŸ¡ Medium | UI/UX |
| 11 | `$HOME` env variable missing on Windows | ğŸŸ¢ Low | Environment |

</details>

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **Dynamic Schema** | Automatically detects column types (`numeric`, `date`, `text`) from any CSV â€” no hardcoded schemas |
| **ETL Pipeline** | Extract â†’ Transform â†’ Load with data validation, deduplication, and upsert strategies |
| **Real-Time Logs** | WebSocket-powered live log streaming during pipeline execution |
| **Dashboard** | KPI cards, execution timeline, and quick-action controls |
| **Analytics** | Data trends, category breakdown, and comparisons via Recharts |
| **Data Explorer** | Browse, search, and paginate loaded records with dynamic columns |
| **CSV Export** | Export processed data as CSV directly from the Data Explorer |
| **Quarantine** | Review invalid/rejected rows with reason codes |
| **History** | Full execution log with duration, status, and row-level metrics |
| **Database Reset** | One-click cleanup of all data, run history, and quarantine files |
| **i18n** | Bilingual support (English / EspaÃ±ol) with persistent language selection |
| **Smart Toggle** | Auto-detect test data toggle disables automatically when a real CSV is uploaded |
| **CLI Mode** | Original command-line interface still fully functional |

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Node.js 18+**

### 1. Clone & Configure

```bash
git clone https://github.com/BR4Y4NEXE/FluxCLI.git
cd FluxCLI
cp .env.example .env
# Edit .env with your SMTP, Slack, and DB settings
```

### 2. Backend

```bash
pip install -r requirements.txt
pip install -r requirements-api.txt
python -m uvicorn backend.main:app --reload --port 8000
```

### 3. Frontend

```bash
cd frontend
npm install
npm run dev
```

Open **http://localhost:5173** â€” the Vite dev server proxies API calls to the backend automatically.

### 4. CLI Mode (Optional)

```bash
# Auto-detect today's file
python etl.py --auto

# Dry run (no DB writes, no notifications)
python etl.py --auto --dry-run

# Process a specific file
python etl.py --file data/input/my_data.csv
```

---

## ğŸ“‚ Project Structure

```
FluxCLI/
â”œâ”€â”€ etl.py                    # Original CLI entry point
â”œâ”€â”€ mock_data_gen.py          # Test data generator
â”œâ”€â”€ Procfile                  # Deployment configuration
â”œâ”€â”€ .python-version           # Python version pinning
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py               # FastAPI application
â”‚   â”œâ”€â”€ database.py           # SQLite: datasets, dataset_schema, pipeline_runs
â”‚   â”œâ”€â”€ models.py             # Pydantic schemas (ColumnSchemaResponse, PaginatedRecords, etc.)
â”‚   â”œâ”€â”€ routes/               # API endpoints (pipeline, data, stats)
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ pipeline_runner.py # Async orchestration layer
â”‚       â””â”€â”€ log_handler.py    # WebSocket log bridge
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.jsx           # Router + sidebar layout
â”‚       â”œâ”€â”€ img/              # Datrix logo (datrix-logo-v3.svg)
â”‚       â”œâ”€â”€ pages/
â”‚       â”‚   â”œâ”€â”€ Dashboard.jsx
â”‚       â”‚   â”œâ”€â”€ Analytics.jsx
â”‚       â”‚   â”œâ”€â”€ DataExplorer.jsx
â”‚       â”‚   â”œâ”€â”€ History.jsx
â”‚       â”‚   â””â”€â”€ Quarantine.jsx
â”‚       â”œâ”€â”€ hooks/            # Custom React hooks (useWebSocket)
â”‚       â”œâ”€â”€ i18n/
â”‚       â”‚   â”œâ”€â”€ LanguageContext.jsx  # React Context for i18n
â”‚       â”‚   â”œâ”€â”€ en.json             # English translations
â”‚       â”‚   â””â”€â”€ es.json             # Spanish translations
â”‚       â””â”€â”€ services/         # API client layer (api.js)
â”œâ”€â”€ src/                      # Core ETL modules
â”‚   â”œâ”€â”€ extractor.py
â”‚   â”œâ”€â”€ transformer.py        # Dynamic column detection (ColumnSchema, TransformResult)
â”‚   â”œâ”€â”€ loader.py             # JSON-based dataset storage
â”‚   â””â”€â”€ notifier.py
â”œâ”€â”€ config/                   # Environment configuration
â”œâ”€â”€ data/                     # Input files + quarantine reports
â”œâ”€â”€ logs/                     # Execution logs
â””â”€â”€ tests/                    # Unit tests
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| **ETL Core** | Python, Pandas, SQLite, dataclasses, python-dateutil |
| **Backend** | FastAPI, Uvicorn, WebSocket |
| **Frontend** | React 19, Vite 6, React Router 7 |
| **Charts** | Recharts |
| **Icons** | Lucide React |
| **Notifications** | SMTP (Email), Slack Webhooks |
| **Testing** | Pytest |

---

## ğŸ“ License

This project is licensed under the MIT License.
